% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\usepackage{appendix} \setcounter{tocdepth}{0}

\title{reduce Users Manual}
\date{October 22, 2014}
\release{X1}
\author{Kennneth Anderson}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\def\PYG@tok@gd{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\def\PYG@tok@gu{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\def\PYG@tok@gt{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.25,0.82}{##1}}}
\def\PYG@tok@gs{\let\PYG@bf=\textbf}
\def\PYG@tok@gr{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\def\PYG@tok@cm{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@vg{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@m{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@mh{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@cs{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\colorbox[rgb]{1.00,0.94,0.94}{##1}}}
\def\PYG@tok@ge{\let\PYG@it=\textit}
\def\PYG@tok@vc{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@il{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@go{\def\PYG@tc##1{\textcolor[rgb]{0.19,0.19,0.19}{##1}}}
\def\PYG@tok@cp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@gi{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\def\PYG@tok@gh{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\def\PYG@tok@ni{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\def\PYG@tok@nl{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\def\PYG@tok@nn{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\def\PYG@tok@no{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\def\PYG@tok@na{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@nb{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@nc{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\def\PYG@tok@nd{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\def\PYG@tok@ne{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@nf{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\def\PYG@tok@si{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\def\PYG@tok@s2{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@vi{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@nt{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\def\PYG@tok@nv{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@s1{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@gp{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\def\PYG@tok@sh{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@ow{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@sx{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\def\PYG@tok@bp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@c1{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@kc{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@c{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@mf{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@err{\def\PYG@bc##1{\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{##1}}}
\def\PYG@tok@kd{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@ss{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\def\PYG@tok@sr{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\def\PYG@tok@mo{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@mi{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@kn{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@o{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\def\PYG@tok@kr{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@s{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@kp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@w{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\def\PYG@tok@kt{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\def\PYG@tok@sc{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@sb{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@k{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@se{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@sd{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index-latex::doc}



\chapter{Introduction}
\label{intro:introduction}\label{intro:reduce-users-manual}\label{intro::doc}
This document is version 1.0 of the \code{reduce} Users Manual. This manual will
describe the usage of \code{reduce} as an application provided by the Gemini Observatory
Astrodata package suite. \code{reduce} is an application that allows users to invoke the
Gemini Recipe System to perform data processing and reduction on one or more
astronomical datasets.

This document presents details on applying \code{reduce} to astronomical datasets,
currently defined as multi-extension FITS (MEF) files, both through the application's
command line interface and the application programming interface (API). Details and
information about the \code{astrodata} package, the Recipe System, and/or the data
processing involved in data reduction are beyond the scope of this document and
will only be engaged when directly pertinent to the operations of \code{reduce}.


\section{Reference Documents}
\label{intro:reference-documents}\begin{itemize}
\item {} 
\emph{The Gemini Recipe System: a dynamic workflow for automated data reduction},
K. Labrie \emph{et al}, SPIE, 2010.

\item {} 
\emph{Developing for Gemini’s extensible pipeline environment}, K. Labrie,
C. Allen, P. Hirst, ADASS, 2011

\item {} 
\emph{Gemini's Recipe System; A publicly available instrument-agnostic pipeline
infrastructure}, K. Labrie et al, ADASS 2013.

\end{itemize}


\section{Overview}
\label{intro:overview}
As an application, \code{reduce} provides interfaces to configure and launch the
Gemini Recipe System, a framework for developing and running configurable data
processing pipelines and which can accommodate processing pipelines for arbitrary
dataset types. In conjunction with the development of \code{astrodata}, Gemini
Observatory has also developed the compatible \code{astrodata\_Gemini} package, the
code base currently providing abstraction of, and processing for, Gemini
Observatory astronomical observations.

In Gemini Observatory's operational environment ``on summit,'' \code{reduce},
\code{astrodata}, and the \code{astrodata\_Gemini} packages provide a currently defined,
near-realtime, quality assurance pipeline, the so-called QAP. \code{reduce} is used
to launch this pipeline on newly acquired data and provide image quality metrics
to observers, who then assess the metrics and apply observational decisions on
telescope operations.

Users unfamiliar with terms and concepts heretofore presented should consult
documentation cited in the previous sections (working on the Recipe System User
Manual).


\section{Glossary}
\label{intro:glossary}\begin{quote}

\textbf{adcc} -- Automatated Data Communication Center. Provides XML-RPC and HTTP
services for pipeline operations. Can be run externally to \code{reduce.} Users
need not know about or invoke the \code{adcc} for \code{reduce} operations.
\code{reduce} will launch an \code{adcc} instance if one is not available. See
Sec. {\hyperref[discuss:adcc]{\emph{The adcc}}} for further discussion on \code{adcc}.

\textbf{astrodata} (or Astrodata) -- part of the \textbf{gemini\_python} package suite
that defines the dataset abstraction layer for the Recipe System.

\textbf{AstroData} -- not to be confused with \textbf{astrodata}, this is the main class
of the \code{astrodata} package, and the one most users and developers will
interact with at a programmatic level.

\textbf{AstroDataType} -- Represents a data classification. A dataset will be
classified by a number of types that describe both the data and its processing
state. The AstroDataTypes are hierarchical, from generic to specific.  For
example, a typical GMOS image might have a set of types like

`GMOS\_S', `GMOS\_IMAGE', `GEMINI', `SIDEREAL', `IMAGE', `GMOS', `GEMINI\_SOUTH',
`GMOS\_RAW', `UNPREPARED', `RAW' (see \textbf{types} below).

\textbf{astrodata\_Gemini} -- the \textbf{gemini\_python} package that provides all
observatory specific definitions of data types, \textbf{recipes}, and associated
\textbf{primitives} for Gemini Observatory data.

\textbf{astrodata\_X} -- conceivably a data reduction package that could reduce
other observatory and telescope data. Under the Astrodata system, it is
entirely possible for the Recipe System to process HST or Keck data, given
the development of an associated package, astrodata\_HST or astrodata\_Keck.
Pipelines and processing functions are defined for the particulars of each
telescope and its various instruments.

\textbf{Descriptor} -- Represents a high-level metadata name. Descriptors allow
access to essential information about the data through a uniform,
instrument-agnostic interface to the FITS headers.

\textbf{gemini\_python} -- A suite of packages comprising \textbf{astrodata},
\textbf{astrodata\_Gemini}, \textbf{astrodata\_FITS}, and \textbf{gempy}, all of which provide
the full functionality needed to run \textbf{Recipe System}  pipelines on
observational datasets.

\textbf{gempy} -- a \textbf{gemini\_python} package comprising functional utilities to
the \textbf{astrodata\_Gemini} package.

\textbf{MEF} -- Multiple Extension FITS, the standard data format not only for
Gemini Observatory but many observatories.

\textbf{primitive} -- A function defined within an \textbf{astrodata\_{[}X{]}} package that
performs actual work on the passed dataset. Primitives observe tightly
controlled interfaces in support of re-use of primitives and recipes for
different types of data, when possible. For example, all primitives called
\code{flatCorrect} must apply the flat field correction appropriate for the data’s
current AstroDataType, and must have the same set of input parameters.

\textbf{recipe} -- Represents the sequence of transformations. A recipe is a
simple text file that enumerates the set and order of \textbf{primitives} that will
process the passed dataset. A \textbf{recipe} is the high-level pipeline definition.
Users can pass recipe names directly to reduce. Essentially, a recipe is a
pipeline.

\textbf{Recipe System} -- The gemin\_python framework that accommodates an arbitrary
number of defined recipes and the primitives

\textbf{reduce} -- The user/caller interface to the Recipe System and its associated
recipes/pipelines.

\textbf{type} or \textbf{typeset} --  Not to be confused with language primitive or
programmatic data types, these are data types defined within an
\textbf{astrodata\_{[}X{]}} package used to describe the kind of observational data that
has been passed to the Recipe System., Eg., GMOS\_IMAGE, NIRI. In this document,
these terms are synonymous with \textbf{AstroDataType} unless otherwise indicated.
\end{quote}


\chapter{User Environment}
\label{userenv::doc}\label{userenv:user-environment}
Once a user has has retrieved the gemini\_python package, available as a tarfile
from the Gemini website (\href{http://gemini.edu}{http://gemini.edu}), and untarred only minor adjustments need
to be made to the user environment in order to make astrodata importable and
allow \code{reduce} to work properly.


\section{Installation}
\label{userenv:installation}\label{userenv:config}
Download the gemini\_python X1 distribution (tar archive), place the tarfile as
desired, and extract the archive:

\begin{Verbatim}[commandchars=\\\{\}]
\$ tar -xvf gemini\_python\_X1.tar.gz
\end{Verbatim}

Next, invoke the usual Distutils command for a standard python module installation:

\begin{Verbatim}[commandchars=\\\{\}]
python setup.py install --prefix=/somewhere/
\end{Verbatim}

This will place executables in \code{/somewhere/bin} and the package modules in
\code{/somewhere/lib/python2.7/site-packages/}.

Users will then need to have \code{/somewhere/bin} in \$PATH and
\code{/somewhere/lib/python2.7/site-packages} in either \$PYTHONPATH or add the
site-packages to sys.path.

\code{reduce} is made available on the command line once the installation is complete.


\section{Test the installation}
\label{userenv:test-the-installation}
Start up the python interpreter and import astrodata:

\begin{Verbatim}[commandchars=\\\{\}]
\$ python
\textgreater{}\textgreater{}\textgreater{} import astrodata
\end{Verbatim}

Next, return to the command line and test that \code{reduce} is reachable
and runs. There may some delay as package modules are byte compiled:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce -h [--help]
\end{Verbatim}

This will print the reduce help to the screen.

If users have Gemini fits files available, they can test that the Recipe System
is functioning as expected with a test recipe provided by the astrodata\_Gemini
package:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce --recipe test\_one /path/to/gemini\_data.fits
\end{Verbatim}

If all is well, users will see something like:

\begin{Verbatim}[commandchars=\\\{\}]
Resetting logger for application: reduce
Logging configured for application: reduce
                       --- reduce, v4890  ---
              Running under astrodata Version GP-X1
All submitted files appear valid
Starting Reduction on set \#1 of 1

  Processing dataset(s):
        gemini\_data.fits

==============================================================================
RECIPE: test\_one
==============================================================================
 PRIMITIVE: showParameters
 -------------------------
 rtf = False
 suffix = '\_scafaasled'
 otherTest = False
 logindent = 3
 logfile = 'reduce.log'
 reducecache = '.reducecache'
 storedcals = 'calibrations/storedcals'
 index = 1
 retrievedcals = 'calibrations/retrievedcals'
 cachedict = \PYGZob{}'storedcals': 'calibrations/storedcals', 'retrievedcals':
              'calibrations/retrievedcals', 'calibrations': 'calibrations',
              'reducecache': '.reducecache'\PYGZcb{}
 loglevel = 'stdinfo'
 calurl\_dict = \PYGZob{}'CALMGR': 'http://fits/calmgr',
                'UPLOADPROCCAL': 'http://fits/upload\_processed\_cal',
                'QAMETRICURL': 'http://fits/qareport',
                'QAQUERYURL': 'http://fits/qaforgui',
                'LOCALCALMGR': 'http://localhost:\%(httpport)d/calmgr/\%(caltype)s'\PYGZcb{}
 logmode = 'standard'
 test = True
 writeInt = False
 calibrations = 'calibrations'
 .
Wrote gemini\_data.fits in output directory
Shutting down proxy servers ...
ADCC is running externally. No proxies to close
reduce exited     on status: 0
\end{Verbatim}

Exit status 0 indicates nominal operations.


\chapter{Interfaces}
\label{interfaces:interfaces}\label{interfaces::doc}

\section{Introduction}
\label{interfaces:introduction}
The \code{reduce} application provides a command line interface and an API, both
of which can configure and launch a Recipe System processing pipeline (a `recipe')
on the input dataset. Control of \code{reduce} and the Recipe System is provided
by a variety of options and switches. Of course, all options and switches
can be accessed and controlled through the API.


\section{Command line interface}
\label{interfaces:command-line-interface}
We begin with the command line help provided by \code{reduce -{-}help}, followed by
further description and discussion of certain non-trivial options that require
detailed explanation.

\begin{Verbatim}[commandchars=\\\{\}]
usage: reduce [options] fitsfile [fitsfile ...]
\end{Verbatim}

positional arguments:

\begin{Verbatim}[commandchars=\\\{\}]
fitsfile [fitsfile ...]
\end{Verbatim}

The {[}options{]} are described in the following sections.


\subsection{Informational switches}
\label{interfaces:informational-switches}\begin{description}
\item[{\textbf{-h, --help}}] \leavevmode
show the help message and exit

\item[{\textbf{-v, --version}}] \leavevmode
show program's version number and exit

\item[{\textbf{-d, --displayflags}}] \leavevmode
Display all parsed option flags and exit.

When specified, this switch will present the user with a table of all
parsed arguments and then exit without running. This allows the user to
check that the configuration is as intended. The table provides a convenient
view of all passed and default values. Unless a user has specified a
recipe (-r, --recipe), `recipename' indicates `None' because at this point,
the Recipe System has not yet been engaged and a default recipe not yet
determined.

Eg.,:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce -d --logmode console fitsfile.fits

--------------------   switches, vars, vals  --------------------

Literals                 var 'dest'              Value
-----------------------------------------------------------------
['--invoked']            :: invoked              :: False
['--addprimset']         :: primsetname          :: None
['-d', '--displayflags'] :: displayflags         :: True
['-p', '--param']        :: userparam            :: None
['--logmode']            :: logmode              :: ['console']
['-r', '--recipe']       :: recipename           :: None
['--throw\_descriptor\_exceptions'] :: throwDescriptorExceptions :: False
['--logfile']            :: logfile              :: reduce.log
['-t', '--astrotype']    :: astrotype            :: None
['--override\_cal']       :: user\_cals            :: None
['--context']            :: running\_contexts     :: None
['--calmgr']             :: cal\_mgr              :: None
['--suffix']             :: suffix               :: None
['--loglevel']           :: loglevel             :: stdinfo
-----------------------------------------------------------------

Input fits file(s):      fitsfile.fits
\end{Verbatim}

\end{description}


\subsection{Configuration Switches, Options}
\label{interfaces:configuration-switches-options}\begin{description}
\item[{\textbf{--addprimset \textless{}PRIMSETNAME\textgreater{}}}] \leavevmode
Add this path to user-supplied primitives for reduction. eg., path to a
primitives module.

\item[{\textbf{--calmgr \textless{}CAL\_MGR\textgreater{}}}] \leavevmode
This is a URL specifying a calibration manager service. A calibration manager
overides Recipe System table. Not available outside Gemini operations.

\item[{\textbf{--context \textless{}RUNNING\_CONTEXTS\textgreater{}}}] \leavevmode
Use \textless{}RUNNING\_CONTEXTS\textgreater{} for primitives sensitive to context. Eg., --context QA
When not specified, the context defaults to `QA'.

\item[{\textbf{--invoked}}] \leavevmode
Boolean indicating that reduce was invoked by the control center.

\item[{\textbf{--logmode \textless{}LOGMODE\textgreater{}}}] \leavevmode
Set logging mode. One of `standard', `console', `quiet', `debug', or `null',
where `console' writes only to screen and `quiet' writes only to the log
file. Default is `standard'.

\item[{\textbf{--logfile \textless{}LOGFILE\textgreater{}}}] \leavevmode
Set the log file name. Default is `reduce.log' in the current directory.

\item[{\textbf{--loglevel \textless{}LOGLEVEL\textgreater{}}}] \leavevmode
Set the verbose level for console logging. One of
`critical', `error', `warning', `status', `stdinfo', `fullinfo', `debug'.
Default is `stdinfo'.

\item[{\textbf{--override\_cal \textless{}USER\_CALS {[}USER\_CALS ...{]}\textgreater{}}}] \leavevmode
The option allows users to provide their own calibrations to \code{reduce}.
Add a calibration to User Calibration Service.
`--override\_cal CALTYPE:CAL\_PATH'
Eg.,:
\begin{quote}

--override\_cal processed\_arc:wcal/gsTest\_arc.fits
\end{quote}

\item[{\textbf{-p \textless{}USERPARAM {[}USERPARAM ...{]}\textgreater{}, --param \textless{}USERPARAM {[}USERPARAM ...{]}\textgreater{}}}] \leavevmode
Set a primitive parameter from the command line. The form `-p par=val' sets
the parameter in the reduction context such that all primitives will `see' it.
The form

\begin{Verbatim}[commandchars=\\\{\}]
-p ASTROTYPE:primitivename:par=val
\end{Verbatim}

sets the parameter such that it applies only when the current reduction type
(type of current reference image) is `ASTROTYPE' and the primitive is
`primitivename'. Separate parameter-value pairs by whitespace:
(eg. `-p par1=val1 par2=val2')

See Sec. {\hyperref[interfaces:userpars]{\emph{Overriding Primitive Parameters}}}, for more information on these values.

\item[{\textbf{-r \textless{}RECIPENAME\textgreater{}, --recipe \textless{}RECIPENAME\textgreater{}}}] \leavevmode
Specify an explicit recipe to be used rather than internally determined by
a dataset's \textless{}ASTROTYPE\textgreater{}. Default is None and later determined by the Recipe
System based on the AstroDataType.

\item[{\textbf{-t \textless{}ASTROTYPE\textgreater{}, --astrotype \textless{}ASTROTYPE\textgreater{}}}] \leavevmode
Run a recipe based on this AstroDataType, which overrides default type or
begins without initial input. Eg., recipes that begin with primitives that
acquire data. \code{reduce} default is None and determined internally.

\item[{\textbf{--suffix \textless{}SUFFIX\textgreater{}}}] \leavevmode
Add `suffix' to output filenames at end of reduction.

\item[{\textbf{--throw\_descriptor\_exceptions}}] \leavevmode
Boolean indicating descriptor exceptions are to be raised. This is a
development switch.

\end{description}


\subsection{Nominal Usage}
\label{interfaces:nominal-usage}
The minimal call for reduce can be

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce \textless{}dataset.fits\textgreater{}
\end{Verbatim}

While this minimal call is available at the Gemini Observatory, if a calibration
service is unavailable to the user -- likely true for most users -- users should
call \code{reduce} on a specified dataset by providing calibration files with the
--overrride\_cal option. For example:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce --override\_cal processed\_arc:wcal/gsTest\_arc.fits \textless{}dataset.fits\textgreater{}
\end{Verbatim}

Such a command for complex processing of data is possible because AstroData
and the Recipe System do all the necessary work in determining how the data are to
be processed, which is critcially based upon the determination of the \emph{typeset}
that applies to that data.

Without any user-specified recipe (-r --recipe), the default recipe is
\code{qaReduce}, which is defined for various AstroDataTypes and currently used at
the summit. For example, the \code{qaReduce} recipe for a GMOS\_IMAGE specifies that
the following primitives are called on the data:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{prepare}
\PYG{n}{addDQ}
\PYG{n}{addVAR}
\PYG{n}{detectSources}
\PYG{n}{measureIQ}
\PYG{n}{measureBG}
\PYG{n}{measureCCAndAstrometry}
\PYG{n}{overscanCorrect}
\PYG{n}{biasCorrect}
\PYG{n}{ADUToElectrons}
\PYG{n}{addVAR}
\PYG{n}{flatCorrect}
\PYG{n}{mosaicDetectors}
\PYG{n}{makeFringe}
\PYG{n}{fringeCorrect}
\PYG{n}{detectSources}
\PYG{n}{measureIQ}
\PYG{n}{measureBG}
\PYG{n}{measureCCAndAstrometry}
\PYG{n}{addToList}
\end{Verbatim}

The point here is not to overwhelm readers with a stack of primitive names, but
to present both the default pipeline processing that the above simple \code{reduce}
command invokes and to demonstrate how much the \code{reduce} interface abstracts
away the complexity of the processing that is engaged with the simplist of
commands.


\subsection{Overriding Primitive Parameters}
\label{interfaces:userpars}\label{interfaces:overriding-primitive-parameters}
In some cases, users may wish to change the functional behaviour of certain
processing steps, i.e. change default behaviour of primitive
functions.

Each primitive has a set of pre-defined parameters, which are used to control
functional behaviour of the primitive. Each defined parameter has a ``user
override'' token, which indicates that a particular parameter may be overridden
by the user. Users can adjust parameter values from the reduce command line with
the option,
\begin{quote}

\textbf{-p, --param}
\end{quote}

If permitted by the ``user override'' token, parameters and values specified
through the \textbf{-p, --param} option will \emph{override} the defined
parameter default value and may alter default behaviour of the primitive
accessing this parameter. A user may pass several parameter-value pairs with
this option.

Eg.:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce -p par1=val1 par2=val2 [par3=val3 ... ] \textless{}fitsfile1.fits\textgreater{}
\end{Verbatim}

For example, some photometry primitives perform source detection on an image.
The `detection threshold' has a defined default, but a user may alter this
parameter default to change the source detection behaviour:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce -p threshold=4.5 \textless{}fitsfile.fits\textgreater{}
\end{Verbatim}


\subsection{The @file facility}
\label{interfaces:the-file-facility}\label{interfaces:atfile}
The reduce command line interface supports what might be called an `at-file'
facility (users and readers familiar with IRAF will recognize this facility).
This facility allows users to provide any and all command line options and flags
to \code{reduce} via in a single acsii text file.

By passing an @file to \code{reduce} on the command line, users can encapsulate all
the options and positional arguments they might wish to specify in a single
@file. It is possible to use multiple @files and even to embed one or more
@files in another. The parser opens all files sequentially and parses
all arguments in the same manner as if they were specified on the command line.
Essentially, an @file is some or all of the command line and parsed identically.

To illustrate the convenience provided by an \href{mailto:'@file}{`@file}`, let us begin with an
example \emph{reduce} command line that has a number of arguments:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce -p GMOS\_IMAGE:contextReport:tpar=100 GMOS\_IMAGE:contextReport:report\_inputs=True
  -r recipe.ArgsTest --context qa S20130616S0019.fits N20100311S0090.fits
\end{Verbatim}

Ungainly, to be sure. Here, two (2) \emph{user parameters} are being specified
with \textbf{-p}, a \emph{recipe} with \textbf{-r}, and a \emph{context} argument is specified
to be \textbf{qa} . This can be wrapped in a plain text @file called
\emph{reduce\_args.par}:

\begin{Verbatim}[commandchars=\\\{\}]
S20130616S0019.fits
N20100311S0090.fits
--param
GMOS\_IMAGE:contextReport:tpar=100
GMOS\_IMAGE:contextReport:report\_inputs=True
-r recipe.ArgsTests
--context qa
\end{Verbatim}

This then turns the previous reduce command line into something a little more
\emph{keyboard friendly}:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce @reduce\_args.par
\end{Verbatim}

The order of these arguments is irrelevant. The parser will figure out what is
what. The above file could be thus written like:

\begin{Verbatim}[commandchars=\\\{\}]
-r recipe.ArgsTests
--param
GMOS\_IMAGE:contextReport:tpar=100
GMOS\_IMAGE:contextReport:report\_inputs=True
--context qa
S20130616S0019.fits
N20100311S0090.fits
\end{Verbatim}

\begin{notice}{note}{Note:}
Comments are accommodated, both line and in-line. `=' signs \emph{may} be
used but this has meaning only for arguments that expect unitary
values. The `=' is entirely unnecessary.

White space is the only significant separator of arguments: spaces,
tabs, newlines are all equivalent when argument parsing. This means
the user can `arrange' their @file for clarity.

Eg., a more readable version of the above file might be written as:

\begin{Verbatim}[commandchars=\\\{\}]
\# reduce parameter file
\# yyyy-mm-dd
\# GDPSG

\# Spec the recipe
-r
    recipe.ArgsTests  \# test recipe

\# primitive parameters here
\# These are 'untyped', i.e. global
--param
    tpar=100
    report\_inputs=True

--context
    qa                \# QA context

S20130616S0019.fits
N20100311S0090.fits
\end{Verbatim}
\end{notice}

All the above  examples of \code{reduce\_args.par} are equivalently parsed. Which,
of course, users may check by adding the \textbf{-d} flag:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce -d @redpars.par

--------------------   switches, vars, vals  --------------------

Literals                      var 'dest'              Value
-----------------------------------------------------------------
['--invoked']                 :: invoked              :: False
['--addprimset']              :: primsetname          :: None
['-d', '--displayflags']      :: displayflags         :: True
['-p', '--param']             :: userparam            :: ['tpar=100', 'report\_inputs=True']
['--logmode']                 :: logmode              :: standard
['-r', '--recipe']            :: recipename           :: ['recipe.ArgTests']
['--throw\_descriptor\_exceptions'] :: throwDescriptorExceptions        :: False
['--logfile']                 :: logfile              :: reduce.log
['-t', '--astrotype']         :: astrotype            :: None
['--override\_cal']            :: user\_cals            :: None
['--context']                 :: running\_contexts     :: ['QA']
['--calmgr']                  :: cal\_mgr              :: None
['--suffix']                  :: suffix               :: None
['--loglevel']                :: loglevel             :: stdinfo
-----------------------------------------------------------------

Input fits file(s):   S20130616S0019.fits
Input fits file(s):   N20100311S0090.fits
\end{Verbatim}


\subsection{Recursive @file processing}
\label{interfaces:recursive-file-processing}
As implemented, the @file facility will recursively handle, and process
correctly, other @file specifications that appear in a passed @file or
on the command line. For example, we may have another file containing a
list of fits files, separating the command line flags from the positional
arguments.

We have a plain text `fitsfiles' containing the line:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{test\PYGZus{}data}\PYG{o}{/}\PYG{n}{S20130616S0019}\PYG{o}{.}\PYG{n}{fits}
\end{Verbatim}

We can indicate that this file is to be consumed with the prefix character
``@'' as well. In this case, the `reduce\_args.par' file could thus appear:

\begin{Verbatim}[commandchars=\\\{\}]
\# reduce test parameter file

@fitsfiles       \# file with fits files

\# AstroDataType
-t GMOS\_IMAGE

\# primitive parameters.
--param
    report\_inputs=True
    tpar=99
    FOO=BAR

\# Spec the recipe
-r recipe.ArgTests
\end{Verbatim}

The parser will open and read the @fitsfiles, consuming those lines in the
same way as any other command line arguments. Indeed, such a file need not only
contain fits files (positional arguments), but other arguments as well. This is
recursive. That is, the @fitsfiles can contain other at-files'', which can contain
other ``at-files'', which can contain ..., \emph{ad infinitum}. These will be processed
serially.

As stipulated earlier, because the @file facility provides arguments equivalent
to those that appear on the command line, employment of this facility means that
a reduce command line could assume the form:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce @parfile @fitsfiles
\end{Verbatim}

or equally:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce @fitsfiles @parfile
\end{Verbatim}

where `parfile' could contain the flags and user parameters, and `fitsfiles'
could contain a list of datasets.

Eg., fitsfiles comprises the one line:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{test\PYGZus{}data}\PYG{o}{/}\PYG{n}{N20100311S0090}\PYG{o}{.}\PYG{n}{fits}
\end{Verbatim}

while parfile holds all other specifications:

\begin{Verbatim}[commandchars=\\\{\}]
\# reduce test parameter file
\# GDPSG

\# AstroDataType
-t GMOS\_IMAGE

\# primitive parameters.
--param
    report\_inputs=True
    tpar=99            \# This is a test parameter
    FOO=BAR            \# This is a test parameter

\# Spec the recipe
-r recipe.ArgTests
\end{Verbatim}


\subsection{Overriding @file values}
\label{interfaces:overriding-file-values}
The \code{reduce} application employs a customized command line parser such that
the command line option

\textbf{-p} or \textbf{--param}

will accumulate a set of parameters \emph{or} override a particular parameter.
This may be seen when a parameter is specified in a user @file and then
specified on the command line. For unitary value arguments, the command line
value will \emph{override} the @file value.

It is further specified that if one or more datasets (i.e. positional arguments)
are passed on the command line, \emph{all fits files appearing as positional arguments}
\emph{in the parameter file will be replaced by the command line arguments.}

Using the parfile above,

Eg. 1)  Accumulate a new parameter:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce @parfile --param FOO=BARSOOM

parsed options:
--------------------
AstroDataType: GMOS\_IMAGE
FITS files:    ['S20130616S0019.fits', 'N20100311S0090.fits']
Parameters:    tpar=100, report\_inputs=True, FOO=BARSOOM
RECIPE:        recipe.ArgsTest
\end{Verbatim}

Eg. 2) Override a parameter in the @file:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce @parfile --param tpar=99

parsed options:
--------------------
AstroDataType: GMOS\_IMAGE
FITS files:    ['S20130616S0019.fits', 'N20100311S0090.fits']
Parameters:    tpar=99, report\_inputs=True
RECIPE:        recipe.ArgsTest
\end{Verbatim}

Eg. 3) Override the recipe:

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce @parfile -r=recipe.FOO

parsed options:
--------------------
AstroDataType:    GMOS\_IMAGE
FITS files:       ['S20130616S0019.fits', 'N20100311S0090.fits']
Parameters:       tpar=100, report\_inputs=True
RECIPE:           recipe.FOO
\end{Verbatim}

Eg. 4) Override a recipe and specify another fits file

\begin{Verbatim}[commandchars=\\\{\}]
\$ reduce @parfile -r=recipe.FOO test\_data/N20100311S0090\_1.fits

parsed options:
--------------------
AstroDataType:    GMOS\_IMAGE
FITS files:       ['test\_data/N20100311S0090\_1.fits']
Parameters:       tpar=100, report\_inputs=True
RECIPE:           recipe.FOO
\end{Verbatim}


\section{Application Programming Interface (API)}
\label{interfaces:application-programming-interface-api}
\begin{notice}{note}{Note:}
The following sections discuss and describe programming interfaces
available on \code{reduce} and the underlying class Reduce.
\end{notice}

The \code{reduce} application is essentially a skeleton script providing the
described command line interface. After parsing the command line, the script
then passes the parsed arguments to its main() function, which in turn calls
the Reduce() class constructor with ``args''. Class Reduce() is defined
in the module \code{coreReduce.py}. \code{reduce} and class Reduce are both
scriptable, as the following discussion will illustrate.


\subsection{reduce.main()}
\label{interfaces:main}\label{interfaces:reduce-main}
The main() function of reduce receives one (1) parameter that is a Namespace
object as returned by a call on ArgumentParser.parse\_args(). Specific to reduce,
the caller can supply this object by a call on the parseUtils.buildParser()
function, which returns a fully defined reduce parser. As usual, the parser
object should then be called with the parse\_args() method to return a valid
reduce parser Namespace. Since there is no interaction with sys.argv, as in
a command line call, all Namespace attributes have only their defined default
values. It is for the caller to set these values as needed.

As the example below demonstrates, once the ``args'' Namespace object is
instantiated, a caller can set any arguments as needed. Bu they must be set
to the correct type. The caller should examine the various ``args'' types to
determine how to set values. For example, args.files is type list, whereas
args.recipename is type string.

Eg.,

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{k+kn}{from} \PYG{n+nn}{astrodata.adutils.reduceutils} \PYG{k+kn}{import} \PYG{n+nb}{reduce}
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{k+kn}{from} \PYG{n+nn}{astrodata.adutils.reduceutils} \PYG{k+kn}{import} \PYG{n}{parseUtils}
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{n}{args} \PYG{o}{=} \PYG{n}{parseUtils}\PYG{o}{.}\PYG{n}{buildParser}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Reduce,v2.0}\PYG{l+s}{"}\PYG{p}{)}\PYG{o}{.}\PYG{n}{parse\PYGZus{}args}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{n}{args}\PYG{o}{.}\PYG{n}{files}
\PYG{g+go}{[]}
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{n}{args}\PYG{o}{.}\PYG{n}{files}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s}{'}\PYG{l+s}{S20130616S0019.fits}\PYG{l+s}{'}\PYG{p}{)}
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{n}{args}\PYG{o}{.}\PYG{n}{recipename} \PYG{o}{=} \PYG{l+s}{"}\PYG{l+s}{recipe.FOO}\PYG{l+s}{"}
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{n+nb}{reduce}\PYG{o}{.}\PYG{n}{main}\PYG{p}{(}\PYG{n}{args}\PYG{p}{)}
\PYG{g+go}{--- reduce, v2.0 ---}
\PYG{g+go}{Starting Reduction on set \PYGZsh{}1 of 1}
\PYG{g+go}{Processing dataset(s):}
\PYG{g+go}{S20130616S0019.fits}
\PYG{g+gp}{...}
\end{Verbatim}

Processing will proceed as usual.


\subsection{Class Reduce and the runr() method}
\label{interfaces:class-reduce-and-the-runr-method}
Class Reduce is defined in \code{astrodata.adutils.reduceutils} module,
\code{coreReduce.py}.

The reduce.main() function serves mainly as a callable for the command line
interface. While main() is callable by users supplying the correct ``args''
parameter (See {\hyperref[interfaces:main]{\emph{reduce.main()}}}), the Reduce() class is also callable and
can be used directly, and more appropriately. Callers need not supply an ``args''
parameter to the class constructor. The instance of Reduce will have all the
same arguments as in a command line scenario, available as attributes on the
instance. Once an instance of Reduce() is instantiated and instance attributes
set as needed, there is one (1) method to call, \textbf{runr()}. This is the only
public method on the class.

\begin{notice}{note}{Note:}
When using Reduce() directly, callers must configure their own logger.
Reduce() does not configure logutils prior to using a logger as
returned by logutils.get\_logger(). The following example will illustrate
how this is easily done. It is \emph{highly recommended} that callers
configure the logger.
\end{notice}

Eg.,

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{k+kn}{from} \PYG{n+nn}{astrodata.adutils.reduceutils.coreReduce} \PYG{k+kn}{import} \PYG{n}{Reduce}
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{n+nb}{reduce} \PYG{o}{=} \PYG{n}{Reduce}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{n+nb}{reduce}\PYG{o}{.}\PYG{n}{files}
\PYG{g+go}{[]}
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{n+nb}{reduce}\PYG{o}{.}\PYG{n}{files}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s}{'}\PYG{l+s}{S20130616S0019.fits}\PYG{l+s}{'}\PYG{p}{)}
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{n+nb}{reduce}\PYG{o}{.}\PYG{n}{files}
\PYG{g+go}{['S20130616S0019.fits']}
\end{Verbatim}

Once an instance of Reduce has been made, callers can then configure logutils
with the appropriate settings supplied on the instance. This is precisely what
\code{reduce} does when it configures logutils.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{k+kn}{from} \PYG{n+nn}{astrodata.adutils} \PYG{k+kn}{import} \PYG{n}{logutils}
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{n}{logutils}\PYG{o}{.}\PYG{n}{config}\PYG{p}{(}\PYG{n}{file\PYGZus{}name}\PYG{o}{=}\PYG{n+nb}{reduce}\PYG{o}{.}\PYG{n}{logfile}\PYG{p}{,} \PYG{n}{mode}\PYG{o}{=}\PYG{n+nb}{reduce}\PYG{o}{.}\PYG{n}{logmode}\PYG{p}{,}
\PYG{g+go}{                    console\PYGZus{}lvl=reduce.loglevel)}
\end{Verbatim}

At this point, the caller is able to call the runr() method on the ``reduce''
instance.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\textgreater{}\textgreater{}\textgreater{} }\PYG{n+nb}{reduce}\PYG{o}{.}\PYG{n}{runr}\PYG{p}{(}\PYG{p}{)}
\PYG{g+go}{All submitted files appear valid}
\PYG{g+go}{Starting Reduction on set \PYGZsh{}1 of 1}
\PYG{g+go}{Processing dataset(s):}
\PYG{g+go}{S20130616S0019.fits}
\PYG{g+gp}{...}
\end{Verbatim}

Processing will then proceed in the usual manner.


\chapter{Supplemental tools}
\label{supptools:supplemental-tools}\label{supptools::doc}
The astrodata package provides a number of command line driven tools, which
users may find helpful in executing reduce on their data.

With the installation and configuration of \code{astrodata} and \code{reduce} comes
some supplemental tools to help users discover information, not only about their
own data, but about the Recipe System, such as available recipes, primitives,
and defined AstroDataTypes.

If the user environment has been configured correctly these applications
will work directly.


\section{listprimitives}
\label{supptools:listprimitives}
The application \code{listprimitives} is available as a command line executable.
This tool displays available primitives for all AstroDataTypes, their parameters,
and defaults. These are the parameters discussed in Sec. {\hyperref[interfaces:userpars]{\emph{Overriding Primitive Parameters}}} that
can be changed by the user with the \textbf{-p, --param} option on reduce. under the
AstroDataTypes. The help describes more options:

\begin{Verbatim}[commandchars=\\\{\}]
\$ listprimitives -h

Usage: listprimitives [options]

Gemini Observatory Primitive Inspection Tool, v1.0 2011

Options:
-h, --help            show this help message and exit
-c, --use-color       apply color output scheme
-i, --info            show more information
-p, --parameters      show parameters
-r, --recipes         list top recipes
-s, --primitive-set   show primitive sets (Astrodata types)
-v, --verbose         set verbose mode
--view-recipe=VIEW\_RECIPE
                      display the recipe
\end{Verbatim}


\subsection{listprimitives information}
\label{supptools:listprimitives-information}
The following section presents examples of the kind of information that
\code{listprimitives} may provide.

Show available recipes:

\begin{Verbatim}[commandchars=\\\{\}]
\$ listprimitives -r

===============================================================================

RECIPES\_Gemini
-------------------------------------------------------------------------------
   1. basicQA
   2. checkQA
   3. makeProcessedArc.GMOS\_SPECT
   4. makeProcessedBias
   5. makeProcessedDark
   6. makeProcessedFlat
   7. makeProcessedFlat.GMOS\_IMAGE
   8. makeProcessedFlat.GMOS\_SPECT
   9. makeProcessedFlat.NIRI\_IMAGE
   10. makeProcessedFringe
   11. qaReduce.GMOS\_IMAGE
   12. qaReduce.GMOS\_SPECT
   13. qaReduce.NIRI\_IMAGE
   14. qaReduceAndStack.GMOS\_IMAGE
   15. qaStack.GMOS\_IMAGE
   16. reduce.F2\_IMAGE
   17. reduce.GMOS\_IMAGE

Subrecipes
-------------------------------------------------------------------------------
   1. biasCorrect
   2. correctWCSToReferenceCatalog
   3. darkCorrect
   4. flatCorrect
   5. lampOnLampOff
   6. makeSky
   7. overscanCorrect
   8. prepare
   9. skyCorrect
   10. standardizeHeaders
   11. thermalEmissionCorrect

===============================================================================
\end{Verbatim}

Users can request display the contents of a particular recipe, as listed above.
This will present the `stack' of primitives that will be called by the
Recipe System when the particular recipe is either specified through the
\code{reduce} command line by the user, or selected internally by the Recipe System
itself.

For example, a user may like to see the primitive stack called by the default
`QA' recipe for GMOS\_IMAGE data. As seen in the above example, these `qa' recipes
are defined for several AstroDataTypes.

Show the primitive stack for the `qa' GMOS\_IMAGE type:

\begin{Verbatim}[commandchars=\\\{\}]
\$ listprimitives --view-recipe qaReduce.GMOS\_IMAGE

===============================================================================
RECIPE: qaReduce.GMOS\_IMAGE
===============================================================================
\# This recipe performs the standardization and corrections needed to convert
\# the raw input science images into a single stacked science image

prepare
addDQ
addVAR(read\_noise=True)
detectSources
measureIQ(display=True)
measureBG
measureCCAndAstrometry
overscanCorrect
biasCorrect
ADUToElectrons
addVAR(poisson\_noise=True)
flatCorrect
mosaicDetectors
makeFringe
fringeCorrect
detectSources
measureIQ(display=True)
measureBG
measureCCAndAstrometry
addToList(purpose=forStack)


===============================================================================
\end{Verbatim}

\code{listprimitives} is in need of refinement and work continues on
building a tool that will present primitives and parameters in a more focused
way, i.e., report only those primitives and parameters relevant to a given
dataset. As it currently stands, users can request that \code{listprimitives}
display primitive parameters (as may be passed to \code{reduce} through the
\textbf{-p} or \textbf{--param} option, Sec. {\hyperref[interfaces:userpars]{\emph{Overriding Primitive Parameters}}}), but this results in a
list of all AstroDataTypes, their primitives and associated parameters.
Admittedly, this list is rather ungainly, but users may see, for example, that
the primitive \code{detectSources} has several user-tunable parameters:

\begin{Verbatim}[commandchars=\\\{\}]
detectSources
    suffix: '\_sourcesDetected'
    centroid\_function: 'moffat'
    threshold: 3.0
    sigma: None
    fwhm: None
    method: 'sextractor'
    max\_sources: 50
\end{Verbatim}

See the discussion in Sec. {\hyperref[interfaces:userpars]{\emph{Overriding Primitive Parameters}}} on command line override of
primitive parameters, and where overriding the `threshold' parameter is dicussed
specifically.


\section{typewalk}
\label{supptools:typewalk}\label{supptools:id1}
As with \code{listprimitives} and \code{reduce}, the command line application \code{typewalk}
becomes available once the user has configured astrodata correctly. \code{typewalk}
examines files in a directory or directory tree and reports the types and status
values through the AstroDataType classification scheme. Running \code{typewalk} on a
directory containing some Gemini datasets will demonstrate what users can expect
to see. If a user has downloaded gemini\_python X1 package with the `test\_data', the
user can move to this directory and run \code{typewalk} on that extensive set of
Gemini datasets.

By default, \code{typewalk} will recurse all subdirectories under the current
directory. Users may specify an explicit directory with the \textbf{-d} or
\textbf{--dir} option.

\code{typewalk} provides the following options {[}\textbf{-h, --help}{]}:

\begin{Verbatim}[commandchars=\\\{\}]
-h, --help            show this help message and exit
-b BATCHNUM, --batch BATCHNUM
                      In shallow walk mode, number of files to process at a
                      time in the current directory. Controls behavior in
                      large data directories. Default = 100.
--calibrations        Show local calibrations (NOT IMPLEMENTED).
-c, --color           Colorize display
-d TWDIR, --dir TWDIR
                      Walk this directory and report types. default is cwd.
-f FILEMASK, --filemask FILEMASK
                      Show files matching regex \textless{}FILEMASK\textgreater{}. Default is all
                      .fits and .FITS files.
-i, --info            Show file meta information.
--keys KEY [KEY ...]  Print keyword values for reported files.Eg., --keys
                      TELESCOP OBJECT
-n, --norecurse       Do not recurse subdirectories.
--or                  Use OR logic on 'types' criteria. If not specified,
                      matching logic is AND (See --types). Eg., --or --types
                      GEMINI\_SOUTH GMOS\_IMAGE will report datasets that are
                      either GEMINI\_SOUTH *OR* GMOS\_IMAGE.
-o OUTFILE, --out OUTFILE
                      Write reported files to this file. Effective only with
                      --types option.
--raise               Raise descriptor exceptions.
--types TYPES [TYPES ...]
                      Find datasets that match only these type criteria.
                      Eg., --types GEMINI\_SOUTH GMOS\_IMAGE will report
                      datasets that are both GEMINI\_SOUTH *and* GMOS\_IMAGE.
--status              Report data processing status only.
--typology            Report data typologies only.
\end{Verbatim}

Files are selected and reported through a regular expression mask which,
by default, finds all ''.fits'' and ''.FITS'' files. Users can change this mask
with the \textbf{-f, --filemask} option.

As the \textbf{--types} option indicates, \code{typewalk} can find and report data that
match specific type criteria. For example, a user might want to find all GMOS
image flats under a certain directory. \code{typewalk} will locate and report all
datasets that would match the AstroDataType, GMOS\_IMAGE\_FLAT.

A user may request that a file be written containing all datasets
matching AstroDataType qualifiers passed by the \textbf{--types} option. An output
file is specified through the \textbf{-o, --out} option. Output files are formatted
so they may be passed \emph{directly to the reduce command line} via that applications
`at-file' (@file) facility. See {\hyperref[interfaces:atfile]{\emph{The @file facility}}} or the reduce help for more on
`at-files'.

Users may select type matching logic with the \textbf{--or} switch. By default,
qualifying logic is AND, i.e. the logic specifies that \emph{all} types must be
present (x AND y); \textbf{--or} specifies that ANY types, enumerated with
\textbf{--types}, may be present (x OR y). \textbf{--or} is only effective when the
\textbf{--types} option is specified with more than one type.

For example, find all GMOS images from Cerro Pachon in the top level
directory and write out the matching files, then run reduce on them
(\textbf{-n} is `norecurse'):

\begin{Verbatim}[commandchars=\\\{\}]
\$ typewalk -n --types GEMINI\_SOUTH GMOS\_IMAGE --out gmos\_images\_south
\$ reduce @gmos\_images\_south
\end{Verbatim}

Find all F2\_SPECT and GMOS\_SPECT datasets in a directory tree:

\begin{Verbatim}[commandchars=\\\{\}]
\$ typewalk --or --types GMOS\_SPECT F2\_SPECT
\end{Verbatim}

This will also report match results to stdout, colourized if requested (\textbf{-c}).


\chapter{Discussion}
\label{discuss:discussion}\label{discuss::doc}

\section{Fits Storage}
\label{discuss:fits-storage}\label{discuss:fitsstore}
blah blah blah Fits Storage blah blah ...


\section{The adcc}
\label{discuss:adcc}\label{discuss:the-adcc}
As a matter of operations, \code{reduce} and the Recipe System depend upon the
services of what is called the \code{adcc}, the Automated Data Communication Center.
The \code{adcc} provides services to pipeline operations through two proxy servers,
an XML-RPC server and an HTTP server. The XML\_RPC server serves calibration
requests made on it, and retrieves calibrations that satisfiy those requests
from the Gemini FITS Store, a service that provides automated calibration
lookup and retrieval.

The \code{adcc} can be run externally and will run continuously until it is
shutdown. Any instances of \code{reduce} (and the Recipe System) will employ
this external instance of the \code{adcc} to service a pipeline's calibration
requests. However, a user of \code{reduce} need not start an instance of the
\code{adcc} nor, indeed, know anytihng about the \code{adcc} \emph{per se}. If one is not
available, an instance of the \code{adcc} will be started by \code{reduce} itself,
and will serve that particular \code{reduce} process and then terminate.

This note is provided should users notice an \code{adcc} process and wonder what
it is.


\section{Future Enhancements}
\label{discuss:future-enhancements}

\subsection{Intelligence}
\label{discuss:intelligence}
One enhancement long imagined is what has been generally termed `intelligence'.
That is, an ability for either \code{reduce} or some utility to automatically do
AstroDataType classification of a set of data, group them appropriately, and
then pass these grouped data to the Recipe System.

As things stand now, it is up to the user to pass commonly typed data to
\code{reduce}. As shown in the previous section, {\hyperref[supptools:typewalk]{\emph{typewalk}}}, \code{typewalk}
can help a user perform this task and create a `ready-to-run' @file that can
be passed directly to \code{reduce}. Properly implemented `intelligence' will
\emph{not} require the user to determine the AstroDataTypes of datasets.


\subsection{Local Calibration Service}
\label{discuss:local-calibration-service}
blahbadeeblah about a local calibration service ...


\chapter{6. Acknowledgments}
\label{ack::doc}\label{ack:acknowledgments}
The Gemini Observatory is operated by the Association of Universities for
Research in Astronomy (AURA), Inc., under a cooperative agreement with the NSF on
behalf of the Gemini partnership: the National Science Foundation
(United States), the Science and Technology Facilities Council (United Kingdom),
the National Research Council (Canada), CONICYT (Chile), the Australian
Research Council (Australia), Ministerio da Ciencia e Tecnologia (Brazil),
and Ministerio de Ciencia, Tecnologia e Innovacion Productiva (Argentina).
% Set up the appendix mode and modify the LaTeX toc behavior
\appendix
\noappendicestocpagenum
\addappheadtotoc

\chapter{\emph{reduce} demo}
\label{appendices/appendix_demo::doc}\label{appendices/appendix_demo:reduce-demo}


\renewcommand{\indexname}{Index}
\printindex
\end{document}
